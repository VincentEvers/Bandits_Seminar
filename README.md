This Seminar was in partial fulfillement of the requirements for the degree of B.Sc. Statistics and Data Science at Ludwig-Maximilians-University.
This repository contains all materials related to my topic of Multi-armed Bandits covered in the Online Learning seminar.

Abstract:

In this report we look at Multi-armed Bandits through the idea of Online Mirror
Descent with estimated Gradients. By choosing negative entropy as the regularizer and
using the softmax link function, we get the exponentiated gradient algorithm. We then
look at its regret bound and show that, with the right learning rate, the algorithm
achieves sublinear regret. Finally a small example is included to show how the weights
change over time and how the regret behaves in practice.

The Code was executed using R version 4.5.1 and the following packages:
- dplyr version 1.1.4
- tidyr version 1.3.1
- ggplot2 version 4.0.1

Hardware:
The code was run on Microsoft Surface 4 laptop using an Intel Core i5-1135G7 CPU running on Windows 11 (64-bit).
